{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15fd4aa-c64f-41a1-94e2-3aeb714dcbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing NaN values with column means.\n",
      "Epoch 5: Train Loss: 1.7275, Test Loss: 1.6459\n",
      "Epoch 10: Train Loss: 1.4971, Test Loss: 1.5532\n",
      "Epoch 15: Train Loss: 1.3512, Test Loss: 1.4256\n",
      "Epoch 20: Train Loss: 1.2372, Test Loss: 1.2799\n",
      "Epoch 25: Train Loss: 1.1457, Test Loss: 1.1396\n",
      "Epoch 30: Train Loss: 1.0680, Test Loss: 1.0195\n",
      "Epoch 35: Train Loss: 0.9929, Test Loss: 0.9232\n",
      "Epoch 40: Train Loss: 0.9322, Test Loss: 0.8461\n",
      "Epoch 45: Train Loss: 0.8690, Test Loss: 0.7817\n",
      "Epoch 50: Train Loss: 0.8148, Test Loss: 0.7266\n",
      "Epoch 55: Train Loss: 0.7621, Test Loss: 0.6764\n",
      "Epoch 60: Train Loss: 0.7190, Test Loss: 0.6306\n",
      "Epoch 65: Train Loss: 0.6768, Test Loss: 0.5885\n",
      "Epoch 70: Train Loss: 0.6456, Test Loss: 0.5510\n",
      "Epoch 75: Train Loss: 0.6088, Test Loss: 0.5180\n",
      "Epoch 80: Train Loss: 0.5809, Test Loss: 0.4885\n",
      "Epoch 85: Train Loss: 0.5559, Test Loss: 0.4612\n",
      "Epoch 90: Train Loss: 0.5297, Test Loss: 0.4369\n",
      "Epoch 95: Train Loss: 0.5096, Test Loss: 0.4157\n",
      "Epoch 100: Train Loss: 0.4880, Test Loss: 0.3966\n",
      "R² Score (Regression): -0.9074\n",
      "F1 Score (Classification): 0.8861\n",
      "Test Accuracy (Classification): 88.62%\n"
     ]
    }
   ],
   "source": [
    "### Step 1: Importing Necessary Libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import torch  # PyTorch for building neural networks\n",
    "import torch.nn as nn  # PyTorch neural network modules\n",
    "import torch.optim as optim  # PyTorch optimization algorithms\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # Data preprocessing tools\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.metrics import r2_score, f1_score  # For evaluating model performance\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "\n",
    "### Step 2: Loading and Preparing Dataset\n",
    "file_path = 'C:/Users/Sanjana Shah/AV GenAI Certification/DL_Pytorch/Assignment/water_quality.csv'  # Path to the dataset file\n",
    "water_quality_df = pd.read_csv(file_path)  # Reading the dataset into a DataFrame\n",
    "\n",
    "\n",
    "### Step 2.1: Handling Missing and Infinite Values\n",
    "\n",
    "# Replacing NaN values with column means to avoid issues with missing data\n",
    "if water_quality_df.isnull().sum().sum() > 0:\n",
    "    print(\"Replacing NaN values with column means.\")\n",
    "    water_quality_df.fillna(water_quality_df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# Replacing Inf values with 0 to avoid computational errors\n",
    "numeric_cols = water_quality_df.select_dtypes(include=[np.number]).columns\n",
    "water_quality_df[numeric_cols] = water_quality_df[numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "### Step 3: Encoding Categorical Columns\n",
    "categorical_columns = ['State', 'District', 'Block', 'Village']\n",
    "# Label encoding categorical features to numerical values\n",
    "for column in categorical_columns:\n",
    "    water_quality_df[column] = LabelEncoder().fit_transform(water_quality_df[column].astype(str))\n",
    "\n",
    "\n",
    "### Step 4: Encoding Target Column\n",
    "# Encoding the categorical target column 'Water Quality Classification'\n",
    "water_quality_df['Water Quality Classification'] = LabelEncoder().fit_transform(water_quality_df['Water Quality Classification'].astype(str))\n",
    "\n",
    "\n",
    "### Step 5: Preparing Features and Targets\n",
    "# Dropping unwanted columns and setting features and targets\n",
    "X = water_quality_df.drop(['WQI', 'Water Quality Classification', 'Well_ID'], axis=1)\n",
    "\n",
    "y_reg = water_quality_df['WQI']  # Regression target\n",
    "\n",
    "y_clf = water_quality_df['Water Quality Classification']  # Classification target\n",
    "\n",
    "\n",
    "### Step 6: Data Scaling\n",
    "scaler = StandardScaler()  # Standardizing features (mean=0, variance=1)\n",
    "X_scaled = scaler.fit_transform(X)  # Applying scaling to the feature data\n",
    "\n",
    "\n",
    "### Step 7: Train/Test Split\n",
    "X_train, X_test, y_train_reg, y_test_reg, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_scaled, y_reg, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "### Step 8: Convert Data to PyTorch Tensors\n",
    "# Converting data to tensors for PyTorch compatibility\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_reg_tensor = torch.tensor(y_train_reg.values, dtype=torch.float32).unsqueeze(1)  # Adding an extra dimension\n",
    "\n",
    "# Regression and Classification targets\n",
    "y_test_reg_tensor = torch.tensor(y_test_reg.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_clf_tensor = torch.tensor(y_train_clf.values, dtype=torch.long)\n",
    "y_test_clf_tensor = torch.tensor(y_test_clf.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "### Step 9: Define Improved Neural Network Model\n",
    "class ImprovedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedNetwork, self).__init__()\n",
    "        # Shared Network Layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        # Separate layers for regression and classification\n",
    "        self.regressor = nn.Linear(64, 1)  # Regression output layer\n",
    "        self.classifier = nn.Linear(64, 5)  # Classification output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared(x)  # Passing data through shared layers\n",
    "        reg_output = self.regressor(shared_features)  # Producing regression output\n",
    "        clf_output = self.classifier(shared_features)  # Producing classification output\n",
    "        return reg_output, clf_output\n",
    "\n",
    "\n",
    "### Step 10: Initializing Model, Losses, and Optimizer\n",
    "model = ImprovedNetwork()  # Initializing the neural network\n",
    "\n",
    "criterion_reg = nn.SmoothL1Loss()  # Regression loss function (better for small errors)\n",
    "criterion_clf = nn.CrossEntropyLoss()  # Classification loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003)  # Using AdamW optimizer\n",
    "\n",
    "# Learning rate scheduler for dynamic adjustment of learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "\n",
    "### Step 11: Training the Model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred_reg_train, y_pred_clf_train = model(X_train_tensor)\n",
    "\n",
    "    # Calculating training losses\n",
    "    train_loss_reg = criterion_reg(y_pred_reg_train, y_train_reg_tensor / 1000)  # Scaling target\n",
    "    train_loss_clf = criterion_clf(y_pred_clf_train, y_train_clf_tensor)\n",
    "    train_loss = train_loss_reg + train_loss_clf\n",
    "\n",
    "    # Backpropagation\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_reg_test, y_pred_clf_test = model(X_test_tensor)\n",
    "        test_loss_reg = criterion_reg(y_pred_reg_test, y_test_reg_tensor / 1000)\n",
    "        test_loss_clf = criterion_clf(y_pred_clf_test, y_test_clf_tensor)\n",
    "        test_loss = test_loss_reg + test_loss_clf\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "\n",
    "### Step 12: Evaluating Model Performance\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred_reg, y_test_pred_clf = model(X_test_tensor)\n",
    "\n",
    "    r2 = r2_score(y_test_reg_tensor.numpy(), y_test_pred_reg.numpy())\n",
    "    _, predicted_classes = torch.max(y_test_pred_clf, 1)\n",
    "    f1 = f1_score(y_test_clf_tensor.numpy(), predicted_classes.numpy(), average='weighted')\n",
    "    accuracy = (predicted_classes == y_test_clf_tensor).sum().item() / len(y_test_clf_tensor)\n",
    "\n",
    "print(f'R² Score (Regression): {r2:.4f}')\n",
    "print(f'F1 Score (Classification): {f1:.4f}')\n",
    "print(f'Test Accuracy (Classification): {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f85e5d-1863-4bdc-808a-57c9dfd846d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
